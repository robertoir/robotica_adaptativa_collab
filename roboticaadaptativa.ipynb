{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roboticaadaptativa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAdf1MngXdcuSmfr7ITlHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertoir/robotica_adaptativa_collab/blob/master/roboticaadaptativa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://gym.openai.com/docs/"
      ],
      "metadata": {
        "id": "YiKndru_90pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gym.openai.com/"
      ],
      "metadata": {
        "id": "NAhTt4aS-DtY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcmeoy1Ggl2z"
      },
      "source": [
        "Frozen-Lake, simula una pista de patinaje sobre hielo. Esta pista está dividida en 16 celdas (4x4), y en algunas de ellas se ha roto el hielo (H). El patinador empieza a patinar en la posición superior izuqierda y us objetivo es alcanzar la posición inferior derecha evitando caer en algún agujero). Si cae en agujero el episodio termina y la recompesa (reward) será cero. Si llega a la celda destino la recompensa será uno y termina el episodio. Al ser 4x4 hay 16 estados se numeran del 0 al 15. Hay un espacio de acciones compuesto de movimientos en cuatro direcciones (left, down, right, up). También hay valla entorno a la pista, si el agente intenta salir de la cuadrícula, robta y queda en la misma celda desde la que lo intentó. Existe un variable (slippery) que permite introducir incertidumbre en el problema. Si el patinador quiere ir hacia una dirección, hay 1/3 de probablidad de que lo logre (33%), 66% que vaya en otra dirección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97lzA_ZKYI5Z"
      },
      "source": [
        "import gym\n",
        "\n",
        "env=gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "env.reset()\n",
        "env.render()\n",
        "is_done=False\n",
        "t=0\n",
        "\n",
        "while not is_done:\n",
        "  accion=env.action_space.sample()\n",
        "  estado,refuerzo,is_done,_=env.step(accion)\n",
        "  env.render()\n",
        "  t=t+1\n",
        "\n",
        "print (\"ultimo estado \",estado)\n",
        "print (\"ultimo refuerzo \",refuerzo)\n",
        "print (\"pasos_ejecutados \",t)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiHVBgGtdaLG"
      },
      "source": [
        "Este código que acabamos de ver, no trabaja con agentes. Reset reinicia la partida, redner muestra el estado del tablero (sistema en este caso), sample escoge una acción de forma aleatoria, step la ejecuta  y devuelve las consecuencias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJfgGyYdRS-"
      },
      "source": [
        "import gym\n",
        "\n",
        "class Agent:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.internalenv=gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "\n",
        "  def select_action(self):\n",
        "    return self.internalenv.action_space.sample()\n",
        "\n",
        "env=gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "env.reset()\n",
        "env.render()\n",
        "is_done=False\n",
        "t=0\n",
        "\n",
        "agent=Agent()\n",
        "\n",
        "while not is_done:\n",
        "  accion=env.action_space.sample()\n",
        "  estado,refuerzo,is_done,_=env.step(accion)\n",
        "  env.render()\n",
        "  t=t+1\n",
        "\n",
        "print (\"ultimo estado \",estado)\n",
        "print (\"ultimo refuerzo \",refuerzo)\n",
        "print (\"pasos_ejecutados \",t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeL5_XKOcvON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f876380-539a-488e-f771-69e5e2dfa05d"
      },
      "source": [
        "env=gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "print (\"espacio de estados \",env.observation_space)\n",
        "#Discrete (16) indica que son 16 estados posibles, desde el 0 al 15\n",
        "#Hay un estado inicial, el cero y 5 estados terminales\n",
        "print (\"mostramos el espacio de estados\")\n",
        "print (env.action_space)\n",
        "#Dispone de un espacio de estados discreto, formado por 4 acciones posibles\n",
        "#izquierda, abajo,derecha y arriba\n",
        "\n",
        "env.reset()\n",
        "print (env.P)\n",
        "\n",
        "nS=env.observation_space.n\n",
        "state=0\n",
        "\n",
        "for next_state in env.P[state][0]:\n",
        "  print (\"info del siguiente estado\")\n",
        "  print (next_state)\n",
        "  probability = next_state[0]\n",
        "  new_state = next_state[1]\n",
        "  reward = next_state[2]\n",
        "  print (\"nuevo estado \",new_state,\" al que transita con probabilidad \",probability,\" y recibe el refuerzo \",reward)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "espacio de estados  Discrete(16)\n",
            "mostramos el espacio de estados\n",
            "Discrete(4)\n",
            "{0: {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 4, 0.0, False)], 2: [(1.0, 1, 0.0, False)], 3: [(1.0, 0, 0.0, False)]}, 1: {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 5, 0.0, True)], 2: [(1.0, 2, 0.0, False)], 3: [(1.0, 1, 0.0, False)]}, 2: {0: [(1.0, 1, 0.0, False)], 1: [(1.0, 6, 0.0, False)], 2: [(1.0, 3, 0.0, False)], 3: [(1.0, 2, 0.0, False)]}, 3: {0: [(1.0, 2, 0.0, False)], 1: [(1.0, 7, 0.0, True)], 2: [(1.0, 3, 0.0, False)], 3: [(1.0, 3, 0.0, False)]}, 4: {0: [(1.0, 4, 0.0, False)], 1: [(1.0, 8, 0.0, False)], 2: [(1.0, 5, 0.0, True)], 3: [(1.0, 0, 0.0, False)]}, 5: {0: [(1.0, 5, 0, True)], 1: [(1.0, 5, 0, True)], 2: [(1.0, 5, 0, True)], 3: [(1.0, 5, 0, True)]}, 6: {0: [(1.0, 5, 0.0, True)], 1: [(1.0, 10, 0.0, False)], 2: [(1.0, 7, 0.0, True)], 3: [(1.0, 2, 0.0, False)]}, 7: {0: [(1.0, 7, 0, True)], 1: [(1.0, 7, 0, True)], 2: [(1.0, 7, 0, True)], 3: [(1.0, 7, 0, True)]}, 8: {0: [(1.0, 8, 0.0, False)], 1: [(1.0, 12, 0.0, True)], 2: [(1.0, 9, 0.0, False)], 3: [(1.0, 4, 0.0, False)]}, 9: {0: [(1.0, 8, 0.0, False)], 1: [(1.0, 13, 0.0, False)], 2: [(1.0, 10, 0.0, False)], 3: [(1.0, 5, 0.0, True)]}, 10: {0: [(1.0, 9, 0.0, False)], 1: [(1.0, 14, 0.0, False)], 2: [(1.0, 11, 0.0, True)], 3: [(1.0, 6, 0.0, False)]}, 11: {0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}, 12: {0: [(1.0, 12, 0, True)], 1: [(1.0, 12, 0, True)], 2: [(1.0, 12, 0, True)], 3: [(1.0, 12, 0, True)]}, 13: {0: [(1.0, 12, 0.0, True)], 1: [(1.0, 13, 0.0, False)], 2: [(1.0, 14, 0.0, False)], 3: [(1.0, 9, 0.0, False)]}, 14: {0: [(1.0, 13, 0.0, False)], 1: [(1.0, 14, 0.0, False)], 2: [(1.0, 15, 1.0, True)], 3: [(1.0, 10, 0.0, False)]}, 15: {0: [(1.0, 15, 0, True)], 1: [(1.0, 15, 0, True)], 2: [(1.0, 15, 0, True)], 3: [(1.0, 15, 0, True)]}}\n",
            "info del siguiente estado\n",
            "(1.0, 0, 0.0, False)\n",
            "nuevo estado  0  al que transita con probabilidad  1.0  y recibe el refuerzo  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input(\"Presione una tecla para continuar...\")\n",
        "#Muestra {estado: {accion:[(Pss',s',rss',terminal)]}}\n",
        "env=gym.make(\"FrozenLake-v0\",is_slippery=True)\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "env.P\n",
        "nS=env.observation_space.n\n",
        "state=10\n",
        "action=1\n",
        "\n",
        "for next_state in env.P[state][action]:\n",
        "  print (\"info del siguiente estado\")\n",
        "  print (next_state)\n",
        "  probability = next_state[0]\n",
        "  new_state = next_state[1]\n",
        "  reward = next_state[2]\n",
        "  print (\"nuevo estado \",new_state,\" al que transita con probabilidad \",probability,\" y recibe el refuerzo \",reward)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LyGW6ICyJM9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "GAMMA = 0.9\n",
        "\n",
        "class Agent:\n",
        "  politica_cambiada=True\n",
        "\n",
        "  def __init__(self):\n",
        "    print (\"Inicializamos la valoración de los estados\", env.observation_space.n)\n",
        "    #solo se podrían inicializar los estados no terminales con un valor distinto de cero..\n",
        "    self.value_function = np.zeros(env.observation_space.n)\n",
        "    #construimos una política determininsta, en cada estado se propone una única acción (las acciones son valores enterores entre 0 y 3)\n",
        "    #la politica inicial sugiere ejecutar la acción cero para todos los estados\n",
        "    self.policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    \n",
        "\n",
        "\n",
        "  def policy_evaluation(self):\n",
        "    #P, nS, nA, policy, gamma=0.9, tol=1e-3\n",
        "\n",
        "    umbral_theta=1e-3\n",
        "    #revisar\n",
        "    self.value_function = np.zeros(env.observation_space.n)\n",
        "\n",
        "    copia_value_function =  np.zeros(env.observation_space.n, dtype=int)\n",
        "    nS=env.observation_space.n\n",
        "    \n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    while True:\n",
        "        for state in range(nS):\n",
        "            #asumimos política determinista, por lo que el primer sumatorio del algoritmo desaparece...\n",
        "            for next_state in env.P[state][self.policy[state]]:\n",
        "                probability = next_state[0]\n",
        "                new_state = next_state[1]\n",
        "                reward = next_state[2]\n",
        "                #print(gamma*probability*value_function[new_state])\n",
        "                copia_value_function[state] += probability*(reward + GAMMA*self.value_function[new_state])\n",
        "            print (\"estado \",state,\" valoración original \",self.value_function[state],\"nueva valoracion \",copia_value_function[state])\n",
        "                \n",
        "        diff = max(np.abs(copia_value_function - self.value_function))\n",
        "        print (\"delta \",diff)\n",
        "        #input (\"pulsa para seguir \")\n",
        "        if diff <= umbral_theta:\n",
        "            self.value_function = copia_value_function.copy()\n",
        "            break\n",
        "        else:\n",
        "            self.value_function = copia_value_function.copy()\n",
        "            #reiniciamos para el sumatorio de la siguiente interaccion\n",
        "            copia_value_function = np.zeros(nS) \n",
        "            #print(updated_value_function, '\\n')\n",
        "            #print(value_function)\n",
        "\n",
        "    ############################\n",
        "    #Descomentar la siguiente linea si es necesario\n",
        "    #return value_function\n",
        "  \n",
        "  def policy_improvement(self):\n",
        "     #P, nS, nA, value_from_policy, policy, gamma=0.9\n",
        "     #definimos el numero de estados y el  numero de acciones...\n",
        "     nS=env.observation_space.n\n",
        "     nA=env.action_space.n\n",
        "     #vamos a construir una nueva politica determinista\n",
        "     self.politica_cambiada=False    \n",
        "     ############################\n",
        "     cambios=1\n",
        "     while (cambios>0):\n",
        "       cambios=0\n",
        "       for state in range(nS):\n",
        "        valoraciones = []\n",
        "        candidatas=[]\n",
        "        old_action=self.policy[state]\n",
        "        for action in range(nA):\n",
        "            value = 0\n",
        "            for next_state in env.P[state][action]:\n",
        "                probability = next_state[0]\n",
        "                new_state = next_state[1]\n",
        "                reward = next_state[2]\n",
        "                value += probability*(reward + GAMMA*self.value_function[new_state])\n",
        "            valoraciones.append(value)\n",
        "            #print (\"accion \",action,\"valoracion \",value)\n",
        "            \n",
        "        #candidatas=actions.index(np.max(actions)) \n",
        "        #np.asarray -->convierte la tupla que devuelve np.where en un array (pero tiene forma de matriz)\n",
        "        #flatten --> convierte la matriz en vector, uniendo por filas\n",
        "        candidatas=np.asarray(np.where(max(valoraciones) == valoraciones)).flatten()\n",
        "        #print (\"valoraciones \",actions)\n",
        "        #print (\"acciones candidatas entre las que escogeremos al azar \",candidatas)       \n",
        "\n",
        "        self.policy[state]=random.choice(candidatas)\n",
        "        #self.policy[state] = np.argmax(actions)\n",
        "        print (\"para el estado \",state,\"sugerimos ahora la accion \",self.policy[state]) \n",
        "        if (np.abs(valoraciones[old_action]-valoraciones[self.policy[state]])>0.001):\n",
        "          print (\"old action \",old_action,\"valoracion \",valoraciones[old_action],\"nueva accion \",self.policy[state],\"valoracion nueva \",valoraciones[self.policy[state]])\n",
        "          cambios=cambios+1\n",
        "          self.politica_cambiada=True\n",
        "        print (\"cambios hechos \",cambios)\n",
        "\n",
        "def train(agent): \n",
        "    #writer = SummaryWriter()\n",
        "    #t = 0\n",
        "    #best_reward = 0.0\n",
        " \n",
        "    #while best_reward < REWARD_THRESHOLD:\n",
        "    #    agent.value_iteration(\n",
        "    while (agent.politica_cambiada):\n",
        "      agent.policy_evaluation()\n",
        "      print (\"termine la valoración de la política, ahora la mejoramos....\")\n",
        "      agent.policy_improvement()\n",
        "      print (\"vemos si procede un nuevo ciclo \",agent.politica_cambiada)\n",
        "      input (\"pulsa para un nuevo ciclo...\")\n",
        "\n",
        "\n",
        "env=gym.make(\"FrozenLake-v0\",is_slippery=False)\n",
        "print (\"espacio de estados \",env.observation_space)\n",
        "print (\"el numero de estados es\", env.observation_space.n)\n",
        "#Discrete (16) indica que son 16 estados posibles, desde el 0 al 15\n",
        "#Hay un estado inicial, el cero y 5 estados terminales\n",
        "print (\"mostramos el espacio de estados\")\n",
        "print (env.action_space)\n",
        "#Dispone de un espacio de estados discreto, formado por 4 acciones posibles\n",
        "#izquierda, abajo,derecha y arriba\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "input (\"pulsa para seguir \")\n",
        "#env.P\n",
        "\n",
        "agent = Agent()\n",
        "train(agent)\n",
        "\n",
        "#t=0\n",
        "\n",
        "#while not is_done:\n",
        "#  accion=env.action_space.sample()\n",
        "#  estado,refuerzo,is_done,_=env.step(accion)\n",
        "#  env.render()\n",
        "#  t=t+1\n",
        "\n",
        "#print (\"ultimo estado \",estado)\n",
        "#print (\"ultimo refuerzo \",refuerzo)\n",
        "#print (\"pasos_ejecutados \",t)\n",
        "\n"
      ],
      "metadata": {
        "id": "8FiZDJffMvDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XQcxYTLc3aN"
      },
      "source": [
        "Esto muestra la matriz con las probabilidades de transición (función de transferecia). Cada acción incluye una lista, donde cada elemento es una tupla que muestra la probabilidad de pasar al siguietne estado, el siguiente estado, la recompensa y si el episodio termina allí o no"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQTEdrOMe6P9"
      },
      "source": [
        "import gym\n",
        "\n",
        "env=gym.make(\"FrozenLake-v0\",is_slippery=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIVm4MKfgiTr"
      },
      "source": [
        "Vemos en este caso la función de transición. Cada acción incluye una lista, donde cada elemento es una tupla que muestra la probabilidad de pasar al siguietne estado, el siguiente estado, la recompensa y si el episodio termina allí o no"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va44670jfBGk"
      },
      "source": [
        "env.P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eui-3MeQfYdl"
      },
      "source": [
        "ESta función de transición implementa que hay solo un tercio (33.33%) de probabilidad de que vayamos al etado de la celda prevista, y dos tercios (66.66%) de probabilidad de que vayamos en direcciones orgonales no deseadas"
      ]
    }
  ]
}